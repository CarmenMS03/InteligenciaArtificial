{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea81f93",
   "metadata": {},
   "source": [
    " ## Puzle de 8 con RL\n",
    " \n",
    "Se aplica el algoritmo Q-learning para diseñar un agente que aprenda a resolver el puzzle del 8. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78d00e",
   "metadata": {},
   "source": [
    "Para poder aplicar Q-learning necesitamos numerar las diferentes acciones y estados para poder crear la q_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5622145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "94a827a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from search import Problem\n",
    "import itertools\n",
    "class Ocho_Puzzle(Problem):\n",
    "    \"\"\"Problema a del 8-puzzle.  Los estados serán tuplas de nueve elementos,\n",
    "    permutaciones de los números del 0 al 8 (el 0 es el hueco). Representan la\n",
    "    disposición de las fichas en el tablero, leídas por filas de arriba a\n",
    "    abajo, y dentro de cada fila, de izquierda a derecha. Las cuatro\n",
    "    acciones del problema las representaremos mediante las cadenas:\n",
    "    \"Mover hueco arriba\", \"Mover hueco abajo\", \"Mover hueco izquierda\" y\n",
    "    \"Mover hueco derecha\", respectivamente.\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, initial, goal=(1, 2, 3, 4, 5, 6, 7, 8, 0)):\n",
    "        \"\"\" Define goal state and initialize a problem \"\"\"\n",
    "        self.goal = goal\n",
    "        Problem.__init__(self, initial, goal)\n",
    "        \n",
    "    \n",
    "    def actions(self,estado):\n",
    "     \n",
    "        pos_hueco=estado.index(0) # busco la posicion del 0\n",
    "        accs=list()\n",
    "        if pos_hueco not in (0,1,2):\n",
    "            accs.append(\"Mover hueco arriba\")\n",
    "        if pos_hueco not in (2,5,8):\n",
    "            accs.append(\"Mover hueco derecha\")\n",
    "        if pos_hueco not in (0,3,6): \n",
    "            accs.append(\"Mover hueco izquierda\") \n",
    "        if pos_hueco not in (6,7,8):\n",
    "            accs.append(\"Mover hueco abajo\")\n",
    "                \n",
    "        return accs     \n",
    "\n",
    "    def result(self,estado,accion):\n",
    "        pos_hueco = estado.index(0)\n",
    "        l = list(estado)\n",
    "        if accion == \"Mover hueco arriba\":\n",
    "            l[pos_hueco] = l[pos_hueco-3]\n",
    "            l[pos_hueco-3] = 0\n",
    "        if accion == \"Mover hueco abajo\":\n",
    "            l[pos_hueco] = l[pos_hueco + 3]\n",
    "            l[pos_hueco + 3] = 0\n",
    "        if accion == \"Mover hueco derecha\":\n",
    "            l[pos_hueco] = l[pos_hueco+1]\n",
    "            l[pos_hueco+1] = 0\n",
    "        if accion == \"Mover hueco izquierda\":\n",
    "            l[pos_hueco] = l[pos_hueco-1]\n",
    "            l[pos_hueco-1] = 0\n",
    "        return tuple(l)\n",
    "    \n",
    "    def h(self, node):\n",
    "        \"\"\" Return the heuristic value for a given state. \"\"\"\n",
    "        return 1\n",
    "    \n",
    "    def check_solvability(self, state):\n",
    "        \"\"\" Checks if the given state is solvable \"\"\"\n",
    "    # The solvability of a configuration can be checked by calculating the Inversion Permutation. \n",
    "    #If the total Inversion Permutation is even then the initial configuration is solvable else the initial configuration is not solvable which means that only 9!/2 initial states lead to a solution.\n",
    "        inversion = 0\n",
    "        for i in range(len(state)):\n",
    "            for j in range(i+1, len(state)):\n",
    "                if (state[i] > state[j]) and state[i] != 0 and state[j]!= 0:\n",
    "                    inversion += 1\n",
    "        return inversion % 2 == 0\n",
    "    \n",
    "    def lista_estados(self,estado):\n",
    "        i = 0;\n",
    "        diccionario = {}\n",
    "        for e in itertools.permutations(estado):\n",
    "            diccionario[e] = i\n",
    "            i = i + 1;\n",
    "        return diccionario  \n",
    "    \n",
    "    def listaAcciones(self):\n",
    "        listaAcciones = {}\n",
    "        listaAcciones[0] =\"Mover hueco arriba\" \n",
    "        listaAcciones[1] =\"Mover hueco abajo\"\n",
    "        listaAcciones[2] =\"Mover hueco derecha\"\n",
    "        listaAcciones[3] =\"Mover hueco izquierda\"\n",
    "        return listaAcciones\n",
    "    \n",
    "    def listaAcciones2(self):\n",
    "        listaAcciones2 = {}\n",
    "        listaAcciones2[\"Mover hueco arriba\"] = 0\n",
    "        listaAcciones2[\"Mover hueco abajo\"] =1\n",
    "        listaAcciones2[\"Mover hueco derecha\"] =2\n",
    "        listaAcciones2[\"Mover hueco izquierda\"] =3\n",
    "        return listaAcciones2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d4ec5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos e inicializamos la Q-Table\n",
    "totalAcciones = 4\n",
    "totalEstados = math.factorial(9)\n",
    "q_table = np.zeros([totalEstados, totalAcciones])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4f3245eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial=(1,2,0,4,5,3,7,8,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ba158298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estado objetivo (1, 2, 3, 4, 5, 6, 7, 8, 0)\n"
     ]
    }
   ],
   "source": [
    "ocho = Ocho_Puzzle(initial)\n",
    "print(\"estado objetivo\",ocho.goal)   \n",
    "ocho.__init__(initial) \n",
    "state = ocho.initial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d88bbcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompensa(state):\n",
    "    if state==ocho.goal:\n",
    "        return 100\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "72c18146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agente_puzle8(episodios=10000, alpha = 0.7, gamma = 0.6, epsilon = 0.8):\n",
    "    \"\"\"Creamos y entrenamos el agente con los parametros dados\"\"\"\n",
    "    state = ocho.initial\n",
    "    listaAcciones2 = ocho.listaAcciones2()\n",
    "    listaAcciones = ocho.listaAcciones()\n",
    "    listaEstados = ocho.lista_estados(state)\n",
    "    exitos=0\n",
    "    for i in range(1, episodios):\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            mejor = listaAcciones2[random.choice(ocho.actions(state))]\n",
    "            #print(\"La accion aleatoria seleccionada es \", listaAcciones[mejor])\n",
    "        else:\n",
    "            mejor = 0\n",
    "            for accion in listaAcciones.keys():\n",
    "                if q_table[listaEstados[state]][accion] > q_table[listaEstados[state]][mejor]:\n",
    "                    mejor = accion\n",
    "                    #print(\"La mejor accion es \", listaAcciones[mejor])\n",
    "\n",
    "        if (listaAcciones[mejor] in ocho.actions(state)):\n",
    "            sig_estado = ocho.result(state , listaAcciones[mejor])\n",
    "            reward = recompensa(sig_estado)\n",
    "        else:\n",
    "            reward = -50 #accion no aplicable\n",
    "            sig_estado =state\n",
    "        old_value = q_table[listaEstados[state]][mejor]\n",
    "        next_max = np.max(q_table[listaEstados[sig_estado]][mejor])\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[listaEstados[state], mejor] = new_value\n",
    "        state = sig_estado\n",
    "        if state == ocho.goal:\n",
    "            #Termino el episodio con éxito. Se ha encontrado la solución\n",
    "            state = ocho.initial\n",
    "            exitos=exitos+1\n",
    "            continue\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode: {i}\")\n",
    "\n",
    "    print(\"Training finished.\\n\") \n",
    "    print(\"La Q-table:\\n\") \n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    print(q_table)\n",
    "    # Métrica del % de veces que ha llegado al estado objetivo. \n",
    "    print(\"Número de éxitos: \", exitos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf75046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFICA LOS PARÁMETROS Y OBSERVA EL APRENDIZAJE\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.8\n",
    "episodios=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# puedes medir el tiempo de aprendizaje \n",
    "\n",
    "agente_puzle8(episodios,alpha,gamma,epsilon)\n",
    "# Generar el mapeo de estados a índices\n",
    "lista_estados = ocho.lista_estados(initial)\n",
    "\n",
    "# Mostrar el mapeo completo\n",
    "for estado, indice in lista_estados.items():\n",
    "    print(f\"Índice en Q-table: {indice} -> Estado: {estado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_agente_puzle8(initial):\n",
    "    \"\"\"Solo explotación del agente sin modificar la tabla Q\"\"\"\n",
    "    exitos=0\n",
    "    fallos=0\n",
    "    state = initial\n",
    "    episodios =100\n",
    "    #listaAcciones2 = ocho.listaAcciones2()\n",
    "    listaAcciones = ocho.listaAcciones()\n",
    "    listaEstados = ocho.lista_estados(state)\n",
    "    for i in range(1, episodios+1):\n",
    "        done =False\n",
    "        while not done:\n",
    "            mejor_accion = 0        \n",
    "            for accion in listaAcciones.keys():\n",
    "                if q_table[listaEstados[state]][accion] > q_table[listaEstados[state]][mejor_accion]:\n",
    "                    mejor_accion = accion                \n",
    "            print(\"La accion aleatoria seleccionada es \", listaAcciones[mejor_accion])\n",
    "            print(\"Estado:\", state)\n",
    "            if (listaAcciones[mejor_accion] in ocho.actions(state)):\n",
    "                state = ocho.result(state , listaAcciones[mejor_accion])\n",
    "                print(\"Estado siguiente:\", state)\n",
    "            else: \n",
    "                #termino con fallo porque la accion no es aplicable\n",
    "                fallos=fallos+1\n",
    "                done = True            \n",
    "            if state == ocho.goal:\n",
    "                #print(\"Termino el episodio con éxito. Se ha encontrado la solución\")\n",
    "                exitos=exitos+1 \n",
    "                done =True \n",
    "                state=initial\n",
    "            if i % 100 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Episodio: {i}\")\n",
    "        \n",
    "    print(\"Evaluación terminada.\\n\") \n",
    "    # Métrica del % de veces que ha llegado al estado objetivo. \n",
    "    print(\"Número de éxitos:\", exitos)\n",
    "    print(\"Número de éxitos:\", (exitos/episodios)*100, \" %\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "estado = (1,2,3,4,5,0,7,8,6)\n",
    "#estado = (1,2,3,4,5,6,7,0,8)\n",
    "evalua_agente_puzle8(estado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23daea-171b-44d7-a43f-78e920628b62",
   "metadata": {},
   "source": [
    "¿Funciona bien el entrenamiento?\n",
    "¿Puedes mejorarlo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477321f",
   "metadata": {},
   "source": [
    "El agente ha aprendido con QLearning a resolver el problema del 8 puzle empezando en un estado inicial fijo y ha tardado un cierto tiempo.\n",
    "Una vez entrenado puedes ver qué tal se comporta (usando sólo la parte de explotación de la matriz Q aprendida y midiendo el porcentaje de éxitos). \n",
    "\n",
    "#### Responde a las siguientes cuestiones:\n",
    "\n",
    "- Justifica de forma razonada si esta aproximación con aprendizaje es mejor que la aproximación con búsqueda heurística de la primera práctica.\n",
    "- Observa la matriz de aprendizaje ¿qué ha aprendido el agente?\n",
    "- Si entrenas el agente para un estado inicial ¿cómo se comporta al evaluarlo con otro estado inicial? \n",
    "- Realiza los cambios necesarios para que el agente aprenda a resolver el problema desde cualquier estado inicial.\n",
    "- Te parece adecuada la función de recompensa utilizada. ¿Crees que se podría mejorar el rendimiento del agente cambiando la recompensa?\n",
    "- Comprueba cómo se comporta el aprendizaje con distintas configuraciones de los parámetros. Indica claramente cuál es la mejor configuración que has encontrado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6410ecd3",
   "metadata": {},
   "source": [
    "#### Justifica de forma razonada si esta aproximación con aprendizaje es mejor que la aproximación con búsqueda heurística de la primera práctica.\n",
    "\n",
    "Ni idea honestamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd8ef9",
   "metadata": {},
   "source": [
    "#### Observa la matriz de aprendizaje ¿qué ha aprendido el agente?\n",
    "\n",
    "Para el análisis de la matriz de aprendizaje primero debemos saber cómo está representada:\n",
    "- Las **columnas** hacen referencia a las acciones que pueden realizarse desde cierto estado (mover hueco arriba, abajo, derecha e izquierda).\n",
    "- Las **filas** representan los distintos estados posibles del puzle (todas las permutaciones de piezas).\n",
    "\n",
    "Se usa la línea [np.set_printoptions(threshold=np.inf)] en la función agente_puzle8() para visualizar la QTable entera y se imprimen los distintos estados del puzle con su índice en la matriz con una lista y un bucle for.\n",
    "Los parámetros se usan con los valores : alpha = 0.5, gamma = 0.9 y epsilon = 0.8. Tras 1000000 episodios de aprendizaje y un tiempo real de ejecución de 1 minutos y 15 segundos, se obtiene una QTable en la que observamos:\n",
    "\n",
    "- Valores negativos: Hacen referencia a acciones que no pueden darse como mover el hueco arriba si está ya en una de las posiciones de la primera fila del tablero. Además, al tener un valor alto de gamma, se consigue que los valores se propaguen mucho y allá distintos valores negativos en la matriz.\n",
    "\n",
    "- Valores positivos: Están asociados a movimiento que llevan al estado objetivo y, al igual que pasaba con los valores neagtivos, hay muy variados por la gran importancia del largo plazo. Un ejemplo muy claro es la solución desde el estado inicial de mover dos veces el hueco hacia abajo: el primer movimiento hacia abajo tiene una recompensa de 89.99982024 (fila 1, columna 2) y el segundo una recompensa de 99.99980927 (fila 2304, columna 2).\n",
    "\n",
    "- Valores 0: Es posible que vengan dados por la gran cantidad de permutaciones posibles y que, a pesar del valor de epsilon de 0.5, no todos los estados sean explorados depués de el número de episodios dado.\n",
    "\n",
    "#### Si entrenas el agente para un estado inicial ¿cómo se comporta al evaluarlo con otro estado inicial? \n",
    "\n",
    "Tal y como se daban los valores de los parámetros anteriormente, realmente depende del estado elegido:\n",
    "- Si elegimos un estado que forme parte de la solución del estado inicial (1,2,3,4,5,0,7,8,6) donde se ha dado el primer movimiento hacia abajo, el agente es capaz de encontrar la solución.\n",
    "- Si elegimos un estado del tablero completamente diferente (1,2,3,4,5,6,7,0,8), el agente no es capaz de resolver el problema. Esto se debe a que la QTable queda principalmente llena de valores 0 pues es muy grande y no se exploran correctamente todas las posibilidades. \n",
    "\n",
    "En la mayor parte de los casos, dado un estado inicial aleatorio y esta matriz basada en el estado inicial anterior, el agente no podría encontrar una solución.\n",
    "\n",
    "#### Realiza los cambios necesarios para que el agente aprenda a resolver el problema desde cualquier estado inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agente_puzle8(episodios=10000, alpha=0.7, gamma=0.6, epsilon=0.8):\n",
    "    listaAcciones2 = ocho.listaAcciones2()\n",
    "    listaAcciones = ocho.listaAcciones()\n",
    "    listaEstados = ocho.lista_estados(ocho.initial)\n",
    "    exitos = 0\n",
    "    \n",
    "    for i in range(1, episodios):\n",
    "        # Seleccionar un estado inicial aleatorio y comprobando que sea resoluble\n",
    "        estado = list(random.sample(range(9), 9))\n",
    "        while not ocho.check_solvability(estado):\n",
    "            estado = list(random.sample(range(9), 9))\n",
    "        estado = tuple(estado)\n",
    "        \n",
    "        # Realizar el episodio\n",
    "        for i in range(1, episodios):\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                mejor = listaAcciones2[random.choice(ocho.actions(estado))]\n",
    "                #print(\"La accion aleatoria seleccionada es \", listaAcciones[mejor])\n",
    "            else:\n",
    "                mejor = 0\n",
    "                for accion in listaAcciones.keys():\n",
    "                    if q_table[listaEstados[estado]][accion] > q_table[listaEstados[estado]][mejor]:\n",
    "                        mejor = accion\n",
    "                        #print(\"La mejor accion es \", listaAcciones[mejor])\n",
    "\n",
    "            if (listaAcciones[mejor] in ocho.actions(estado)):\n",
    "                sig_estado = ocho.result(estado , listaAcciones[mejor])\n",
    "                reward = recompensa(sig_estado)\n",
    "            else:\n",
    "                reward = -50 #accion no aplicable\n",
    "                sig_estado =estado\n",
    "            \n",
    "            old_value = q_table[listaEstados[estado]][mejor]\n",
    "            next_max = np.max(q_table[listaEstados[sig_estado]][mejor])\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[listaEstados[estado], mejor] = new_value\n",
    "            estado = sig_estado\n",
    "            if estado == ocho.goal:\n",
    "                #Termino el episodio con éxito. Se ha encontrado la solución\n",
    "                estado = ocho.initial\n",
    "                exitos=exitos+1\n",
    "                continue\n",
    "            if i % 100 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Episode: {i}\")\n",
    "            \n",
    "            # Contador de éxitos\n",
    "            exitos += 1\n",
    "    \n",
    "\n",
    "    print(\"Training finished.\\n\") \n",
    "    print(\"La Q-table:\\n\") \n",
    "    print(q_table)\n",
    "    # Métrica del % de veces que ha llegado al estado objetivo. \n",
    "    print(\"Número de éxitos: \", exitos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.5\n",
    "episodios=500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6948b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = (1, 2, 0, 4, 5, 3, 7, 8, 6)\n",
    "ocho = Ocho_Puzzle(initial)\n",
    "totalAcciones = 4\n",
    "totalEstados = math.factorial(9)\n",
    "q_table = np.zeros([totalEstados, totalAcciones])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f809083",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# puedes medir el tiempo de aprendizaje \n",
    "agente_puzle8(episodios, alpha, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91767e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estado = (1,2,3,4,5,0,7,8,6)\n",
    "estado = (1,2,3,4,5,6,7,0,8)\n",
    "evalua_agente_puzle8(estado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2570aa2",
   "metadata": {},
   "source": [
    "No lo consigo la verdad no entiendo encima tarda un huevoooo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f644257",
   "metadata": {},
   "source": [
    "#### Te parece adecuada la función de recompensa utilizada. ¿Crees que se podría mejorar el rendimiento del agente cambiando la recompensa?\n",
    "\n",
    "Con el objetivo de que encuentre la solución más óptima, se penaliza cada paso con un -1 consiguiendo que el agente minimice el número de acciones hasta la solución maximizando la recompensa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e5fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompensa(state):\n",
    "    if state==ocho.goal:\n",
    "        return 100\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a637bea1",
   "metadata": {},
   "source": [
    "#### Comprueba cómo se comporta el aprendizaje con distintas configuraciones de los parámetros. Indica claramente cuál es la mejor configuración que has encontrado.\n",
    "\n",
    "Se hace un análisis de la modificación de los parámetros sobre las funciones dadas inicialmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a06368c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros([totalEstados, totalAcciones])   \n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.8\n",
    "episodios=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "agente_puzle8(episodios,alpha,gamma,epsilon)\n",
    "lista_estados = ocho.lista_estados(initial)\n",
    "\n",
    "for estado, indice in lista_estados.items():\n",
    "    print(f\"Índice en Q-table: {indice} -> Estado: {estado}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b5654b",
   "metadata": {},
   "source": [
    "- Primer caso (α = 0.5; γ = 0.9; ε=0.8):\n",
    "- Segundo caso (α = 0.7; γ = 0.9; ε=0.8):\n",
    "- Tercer caso (α = 0.5; γ = 0.5; ε=0.8):\n",
    "- Cuarto caso (α = 0.7; γ = 0.5; ε=0.8):\n",
    "- Quinto caso (α = 0.5; γ = 0.5; ε=0.5):\n",
    "- Sexto caso (α = 0.5; γ = 0.5; ε=0.2):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
